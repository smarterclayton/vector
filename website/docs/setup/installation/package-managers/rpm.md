---
title: Install Vector via RPM
sidebar_label: RPM
description: Install Vector through the RPM package manager
---

import CodeExplanation from '@site/src/components/CodeExplanation';
import CodeHeader from '@site/src/components/CodeHeader';
import Jump from '@site/src/components/Jump';
import SVG from 'react-inlinesvg';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Vector can be installed through the [RPM package manager][urls.rpm] which is
generally used on CentOS.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/setup/installation/package-managers/rpm.md.erb
-->

## Install

<Tabs
  centered={true}
  className="rounded"
  defaultValue="rpm"
  values={[{"label":"RPM","value":"rpm"}]}>
<TabItem value="rpm">
<Tabs
  block={true}
  defaultValue="rpm-daemon"
  values={[{"label":"Daemon Strategy","value":"rpm-daemon"},{"label":"Service Strategy","value":"rpm-service"}]}>

<TabItem value="rpm-daemon">

<SVG src="/img/deployment-strategies-docker-daemon.svg" />

As shown in the diagram above, the daemon deployment strategy is designed for
data collection on a single host. Vector is deplyed in it's own container,
collecting and forwarding all data on the host.

---

<div className="steps steps--h3">

<Tabs
  centered={true}
  className="rounded"
  defaultValue="arm64"
  values={[{"label":"ARM64","value":"arm64"},{"label":"ARMv7","value":"armv7"},{"label":"x86_64","value":"x86_64"}]}>

<TabItem value="arm64">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-aarch64.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-aarch64.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
<TabItem value="armv7">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-armv7hl.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-armv7hl.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
<TabItem value="x86_64">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-x86_64.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-x86_64.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "docker" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`docker` source][docs.sources.docker] ingests data through the [Docker engine daemon][urls.docker_daemon] and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
</Tabs>

</div>
</TabItem>

<TabItem value="rpm-service">

_service.md.erb

---

<div className="steps steps--h3">

<Tabs
  centered={true}
  className="rounded"
  defaultValue="arm64"
  values={[{"label":"ARM64","value":"arm64"},{"label":"ARMv7","value":"armv7"},{"label":"x86_64","value":"x86_64"}]}>

<TabItem value="arm64">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-aarch64.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-aarch64.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
<TabItem value="armv7">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-armv7hl.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-armv7hl.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
<TabItem value="x86_64">

1.  ### Download the Vector `.rpm` file

    ```bash
    curl -O https://packages.timber.io/vector/0.8.X/vector-x86_64.rpm
    ```

    [Looking for a specific version?][docs.package_managers.rpm#versions]

2.  ### Install the Vector `.rpm` package directly

    ```bash
    sudo rpm -i vector-x86_64.rpm
    ```

3. ### Configure Vector

   **Where would you like to send your data?**

   <Tabs
     block={true}
     select={true}
     defaultValue="console"
     values={[{"label":"AWS Cloudwatch Logs","value":"aws_cloudwatch_logs"},{"label":"AWS Kinesis Firehose","value":"aws_kinesis_firehose"},{"label":"AWS Kinesis Data Streams","value":"aws_kinesis_streams"},{"label":"AWS S3","value":"aws_s3"},{"label":"Blackhole","value":"blackhole"},{"label":"Clickhouse","value":"clickhouse"},{"label":"Console","value":"console"},{"label":"Elasticsearch","value":"elasticsearch"},{"label":"File","value":"file"},{"label":"GCP Cloud Storage (GCS)","value":"gcp_cloud_storage"},{"label":"GCP PubSub","value":"gcp_pubsub"},{"label":"GCP Stackdriver Logging","value":"gcp_stackdriver_logging"},{"label":"Honeycomb","value":"honeycomb"},{"label":"HTTP","value":"http"},{"label":"Humio Logs","value":"humio_logs"},{"label":"Kafka","value":"kafka"},{"label":"LogDNA","value":"logdna"},{"label":"Loki","value":"loki"},{"label":"New Relic Logs","value":"new_relic_logs"},{"label":"Papertrail","value":"papertrail"},{"label":"Apache Pulsar","value":"pulsar"},{"label":"Sematext Logs","value":"sematext_logs"},{"label":"Socket","value":"socket"},{"label":"Splunk HEC","value":"splunk_hec"},{"label":"Vector","value":"vector"}]}>
   <TabItem value="aws_cloudwatch_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_cloudwatch_logs" # required
     inputs = ["in"] # required
     group_name = "group-name" # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "{{ host }}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_cloudwatch_logs` sink][docs.sinks.aws_cloudwatch_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's CloudWatch Logs service][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_firehose">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_firehose" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_firehose` sink][docs.sinks.aws_kinesis_firehose] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Firehose][urls.aws_kinesis_data_firehose] via the [`PutRecordBatch` API endpoint](https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_kinesis_streams">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_kinesis_streams" # required
     inputs = ["in"] # required
     region = "us-east-1" # required, required when endpoint = ""
     stream_name = "my-stream" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_kinesis_streams` sink][docs.sinks.aws_kinesis_streams] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's Kinesis Data Stream service][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="aws_s3">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "aws_s3" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     region = "us-east-1" # required, required when endpoint = ""

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`aws_s3` sink][docs.sinks.aws_s3] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Amazon Web Service's S3 service][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="blackhole">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "blackhole" # required
     inputs = ["in"] # required
     print_amount = 1000 # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`blackhole` sink][docs.sinks.blackhole] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="clickhouse">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "clickhouse" # required
     inputs = ["in"] # required
     host = "http://localhost:8123" # required
     table = "mytable" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`clickhouse` sink][docs.sinks.clickhouse] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="console">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "console" # required
     inputs = ["in"] # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`console` sink][docs.sinks.console] [streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="elasticsearch">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "elasticsearch" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`elasticsearch` sink][docs.sinks.elasticsearch] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="file">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "file" # required
     inputs = ["in"] # required
     path = "vector-%Y-%m-%d.log" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`file` sink][docs.sinks.file] [streams](#streaming) [`log`][docs.data-model.log] events to a file.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_cloud_storage">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "gcp_cloud_storage" # required
     inputs = ["in"] # required
     bucket = "my-bucket" # required
     compression = "gzip" # required
     credentials_path = "/path/to/credentials.json" # required

     # Encoding
     encoding.codec = "ndjson" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_cloud_storage` sink][docs.sinks.gcp_cloud_storage] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Cloud Storage service](https://cloud.google.com/storage) via the [XML Interface](https://cloud.google.com/storage/docs/xml-api/overview).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_pubsub">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_pubsub" # required
     inputs = ["in"] # required
     project = "vector-123456" # required
     topic = "this-is-a-topic" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_pubsub` sink][docs.sinks.gcp_pubsub] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Pubsub service][urls.gcp_pubsub] via the [REST Interface][urls.gcp_pubsub_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="gcp_stackdriver_logging">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "gcp_stackdriver_logging" # required
     inputs = ["in"] # required
     credentials_path = "/path/to/credentials.json" # required
     log_id = "vector-logs" # required
     project_id = "vector-123456" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`gcp_stackdriver_logging` sink][docs.sinks.gcp_stackdriver_logging] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Google Cloud Platform's Stackdriver Logging service][urls.gcp_stackdriver_logging] via the [REST Interface][urls.gcp_stackdriver_logging_rest].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="honeycomb">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "honeycomb" # required
     inputs = ["in"] # required
     api_key = "${HONEYCOMB_API_KEY}" # required
     dataset = "my-honeycomb-dataset" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`honeycomb` sink][docs.sinks.honeycomb] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Honeycomb][urls.honeycomb] via the [batch events API][urls.honeycomb_batch].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="http">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "http" # required
     inputs = ["in"] # required
     uri = "https://10.22.212.22:9000/endpoint" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`http` sink][docs.sinks.http] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="humio_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "humio_logs" # required
     inputs = ["in"] # required
     token = "${HUMIO_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`humio_logs` sink][docs.sinks.humio_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Humio][urls.humio] via the [HEC API][urls.humio_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="kafka">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "kafka" # required
     inputs = ["in"] # required
     bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # required
     key_field = "user_id" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`kafka` sink][docs.sinks.kafka] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="logdna">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "logdna" # required
     inputs = ["in"] # required
     api_key = "${LOGDNA_API_KEY}" # required
     hostname = "my-local-machine" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`logdna` sink][docs.sinks.logdna] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [LogDna][urls.logdna]'s HTTP Ingestion API.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="loki">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "loki" # required
     inputs = ["in"] # required
     endpoint = "http://localhost:3100" # required

     # Labels
     labels.key = "value" # example
     labels.key = "{{ event_field }}" # example
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`loki` sink][docs.sinks.loki] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Loki][urls.loki].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="new_relic_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "new_relic_logs" # required
     inputs = ["in"] # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`new_relic_logs` sink][docs.sinks.new_relic_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [New Relic's log service][urls.new_relic] via their [log API][urls.new_relic_log_api].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="papertrail">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "papertrail" # required
     inputs = ["in"] # required
     endpoint = "logs.papertrailapp.com:12345" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`papertrail` sink][docs.sinks.papertrail] [streams](#streaming) [`log`][docs.data-model.log] events to [Papertrail](https://www.papertrail.com/) via [Syslog](https://help.papertrailapp.com/kb/how-it-works/http-api/#submitting-log-messages).
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="pulsar">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     address = "127.0.0.1:6650" # required
     topic = "topic-1234" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`pulsar` sink][docs.sinks.pulsar] [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Pulsar][urls.pulsar] via the [Pulsar protocol][urls.pulsar_protocol].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="sematext_logs">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "sematext_logs" # required
     inputs = ["in"] # required
     token = "${SEMATEXT_TOKEN}" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`sematext_logs` sink][docs.sinks.sematext_logs] [batches](#buffers--batches) [`log`][docs.data-model.log] events to [Sematext][urls.sematext] via the [Elasticsearch API][urls.sematext_es].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="socket">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "socket" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required, required when mode = "tcp"
     mode = "tcp" # required
     path = "/path/to/socket" # required, required when mode = "unix"

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`socket` sink][docs.sinks.socket] [streams](#streaming) [`log`][docs.data-model.log] events to a socket, such as a TCP or Unix domain socket.
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="splunk_hec">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     # General
     type = "splunk_hec" # required
     inputs = ["in"] # required
     host = "http://my-splunk-host.com" # required
     token = "${SPLUNK_HEC_TOKEN}" # required

     # Encoding
     encoding.codec = "json" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`splunk_hec` sink][docs.sinks.splunk_hec] [batches](#buffers--batches) [`log`][docs.data-model.log] events to a [Splunk's HTTP Event Collector][urls.splunk_hec].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   <TabItem value="vector">

   <CodeHeader icon="info" text="adjust the values as necessary" />

   ```bash
   echo '
   [sources.in]
     type = "http" # required
     address = "0.0.0.0:80" # required

   [sinks.out]
     type = "vector" # required
     inputs = ["in"] # required
     address = "92.12.333.224:5000" # required
   ' > /etc/vector/vector.toml
   ```

   <CodeExplanation>

   * The the [`http` source][docs.sources.http] ingests data through the HTTP protocol and [outputs `log` events](#output).
   * The the [`vector` sink][docs.sinks.vector] [streams](#streaming) [`log`][docs.data-model.log] events to another downstream [`vector` source][docs.sources.vector].
   * The `/etc/vector/vector.toml` file is the [Vector configuration file][docs.configuration]
     that we'll pass in the next step.

   </CodeExplanation>

   </TabItem>
   </Tabs>

4.  ### Start Vector

    ```bash
    sudo systemctl start vector
    ```

</TabItem>
</Tabs>

</div>
</TabItem>
</Tabs>
</TabItem>
</Tabs>

## Configuring

The Vector configuration file is placed in:

```text
etc/vector/vector.toml
```

A full spec is located at `/etc/vector/vector.spec.toml` and examples are
located in `/etc/vector/examples/*`. You can learn more about configuring
Vector in the [Configuration][docs.configuration] section.

## Deploying

How you deploy Vector is largely dependent on your use case and environment.
Please see the [deployment section][docs.deployment] for more info on how to
deploy Vector.

## Administering

Vector can be managed through the [Systemd][urls.systemd] service manager:

<Jump to="/docs/administration/">Administration</Jump>

## Uninstalling

```bash
sudo rpm -e vector
```

## Updating

Follow the [install](#install) steps again, downloading the latest version of
Vector.

## Package

### Architectures

Vector's RPM packages are multi-arch and support the
x86_64
architectures. The architecture name is prepended to the artifact file name.

### Versions

Vector's RPM packages can be downloaded with the following URLs. Note that
Vector maintains special URLS that are automatically updated whenever Vector is
[released][urls.vector_releases]:

<Tabs
  className="mini"
  defaultValue="ARM64"
  values={[{"label":"ARM64","value":"ARM64"},{"label":"ARMv7","value":"ARMv7"},{"label":"x86_64","value":"x86_64"}]}>
<TabItem value="ARM64">

| Version          | URL                                                                                               |
|:-----------------|:--------------------------------------------------------------------------------------------------|
| Latest major     | `https://packages.timber.io/vector/latest/vector-aarch64.rpm`               |
| Latest minor     | `https://packages.timber.io/vector/<MAJOR>.X/vector-aarch64.rpm`            |
| Latest patch     | `https://packages.timber.io/vector/<MAJOR.MINOR>.X/vector-aarch64.rpm`      |
| Specific version | `https://packages.timber.io/vector/<MAJOR.MINOR.PATCH>/vector-aarch64.rpm`  |
| Latest nightly   | `https://packages.timber.io/vector/nightly/latest/vector-aarch64.rpm`       |
| Specific nightly | `https://packages.timber.io/vector/nightly/<YYYY-MM-DD>/vector-aarch64.rpm` |

</TabItem>
<TabItem value="ARMv7">

| Version          | URL                                                                                               |
|:-----------------|:--------------------------------------------------------------------------------------------------|
| Latest major     | `https://packages.timber.io/vector/latest/vector-armv7hl.rpm`               |
| Latest minor     | `https://packages.timber.io/vector/<MAJOR>.X/vector-armv7hl.rpm`            |
| Latest patch     | `https://packages.timber.io/vector/<MAJOR.MINOR>.X/vector-armv7hl.rpm`      |
| Specific version | `https://packages.timber.io/vector/<MAJOR.MINOR.PATCH>/vector-armv7hl.rpm`  |
| Latest nightly   | `https://packages.timber.io/vector/nightly/latest/vector-armv7hl.rpm`       |
| Specific nightly | `https://packages.timber.io/vector/nightly/<YYYY-MM-DD>/vector-armv7hl.rpm` |

</TabItem>
<TabItem value="x86_64">

| Version          | URL                                                                                               |
|:-----------------|:--------------------------------------------------------------------------------------------------|
| Latest major     | `https://packages.timber.io/vector/latest/vector-x86_64.rpm`               |
| Latest minor     | `https://packages.timber.io/vector/<MAJOR>.X/vector-x86_64.rpm`            |
| Latest patch     | `https://packages.timber.io/vector/<MAJOR.MINOR>.X/vector-x86_64.rpm`      |
| Specific version | `https://packages.timber.io/vector/<MAJOR.MINOR.PATCH>/vector-x86_64.rpm`  |
| Latest nightly   | `https://packages.timber.io/vector/nightly/latest/vector-x86_64.rpm`       |
| Specific nightly | `https://packages.timber.io/vector/nightly/<YYYY-MM-DD>/vector-x86_64.rpm` |

</TabItem>
</Tabs>


### Source Files

Vector's RPM source files are located in
[Vector's repo][urls.vector_rpm_source_files].


[docs.configuration]: /docs/setup/configuration/
[docs.data-model.log]: /docs/about/data-model/log/
[docs.data-model.metric]: /docs/about/data-model/metric/
[docs.deployment]: /docs/setup/deployment/
[docs.package_managers.rpm#versions]: /docs/setup/installation/package-managers/rpm/#versions
[docs.sinks.aws_cloudwatch_logs]: /docs/reference/sinks/aws_cloudwatch_logs/
[docs.sinks.aws_kinesis_firehose]: /docs/reference/sinks/aws_kinesis_firehose/
[docs.sinks.aws_kinesis_streams]: /docs/reference/sinks/aws_kinesis_streams/
[docs.sinks.aws_s3]: /docs/reference/sinks/aws_s3/
[docs.sinks.blackhole]: /docs/reference/sinks/blackhole/
[docs.sinks.clickhouse]: /docs/reference/sinks/clickhouse/
[docs.sinks.console]: /docs/reference/sinks/console/
[docs.sinks.elasticsearch]: /docs/reference/sinks/elasticsearch/
[docs.sinks.file]: /docs/reference/sinks/file/
[docs.sinks.gcp_cloud_storage]: /docs/reference/sinks/gcp_cloud_storage/
[docs.sinks.gcp_pubsub]: /docs/reference/sinks/gcp_pubsub/
[docs.sinks.gcp_stackdriver_logging]: /docs/reference/sinks/gcp_stackdriver_logging/
[docs.sinks.honeycomb]: /docs/reference/sinks/honeycomb/
[docs.sinks.http]: /docs/reference/sinks/http/
[docs.sinks.humio_logs]: /docs/reference/sinks/humio_logs/
[docs.sinks.kafka]: /docs/reference/sinks/kafka/
[docs.sinks.logdna]: /docs/reference/sinks/logdna/
[docs.sinks.loki]: /docs/reference/sinks/loki/
[docs.sinks.new_relic_logs]: /docs/reference/sinks/new_relic_logs/
[docs.sinks.papertrail]: /docs/reference/sinks/papertrail/
[docs.sinks.pulsar]: /docs/reference/sinks/pulsar/
[docs.sinks.sematext_logs]: /docs/reference/sinks/sematext_logs/
[docs.sinks.socket]: /docs/reference/sinks/socket/
[docs.sinks.splunk_hec]: /docs/reference/sinks/splunk_hec/
[docs.sinks.vector]: /docs/reference/sinks/vector/
[docs.sources.docker]: /docs/reference/sources/docker/
[docs.sources.http]: /docs/reference/sources/http/
[docs.sources.vector]: /docs/reference/sources/vector/
[urls.aws_cw_logs]: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html
[urls.aws_kinesis_data_firehose]: https://aws.amazon.com/kinesis/data-firehose/
[urls.aws_kinesis_data_streams]: https://aws.amazon.com/kinesis/data-streams/
[urls.aws_s3]: https://aws.amazon.com/s3/
[urls.clickhouse]: https://clickhouse.yandex/
[urls.clickhouse_http]: https://clickhouse.yandex/docs/en/interfaces/http/
[urls.docker_daemon]: https://docs.docker.com/engine/docker-overview/#the-docker-daemon
[urls.elasticsearch]: https://www.elastic.co/products/elasticsearch
[urls.gcp_pubsub]: https://cloud.google.com/pubsub/
[urls.gcp_pubsub_rest]: https://cloud.google.com/pubsub/docs/reference/rest/
[urls.gcp_stackdriver_logging]: https://cloud.google.com/logging/docs/reference/v2/rest/
[urls.gcp_stackdriver_logging_rest]: https://cloud.google.com/logging/
[urls.honeycomb]: https://honeycomb.io
[urls.honeycomb_batch]: https://docs.honeycomb.io/api/events/#batched-events
[urls.humio]: https://humio.com
[urls.humio_hec]: https://docs.humio.com/integrations/data-shippers/hec/
[urls.kafka]: https://kafka.apache.org/
[urls.kafka_protocol]: https://kafka.apache.org/protocol
[urls.logdna]: https://logdna.com/
[urls.loki]: https://grafana.com/oss/loki/
[urls.new_relic]: https://newrelic.com/
[urls.new_relic_log_api]: https://docs.newrelic.com/docs/logs/new-relic-logs/log-api/introduction-log-api
[urls.pulsar]: https://pulsar.apache.org/
[urls.pulsar_protocol]: https://pulsar.apache.org/docs/en/develop-binary-protocol/
[urls.rpm]: https://rpm.org/
[urls.sematext]: https://sematext.com
[urls.sematext_es]: https://sematext.com/docs/logs/index-events-via-elasticsearch-api/
[urls.splunk_hec]: http://dev.splunk.com/view/event-collector/SP-CAAAE6M
[urls.standard_streams]: https://en.wikipedia.org/wiki/Standard_streams
[urls.systemd]: https://www.freedesktop.org/wiki/Software/systemd/
[urls.vector_releases]: https://vector.dev/releases/latest
[urls.vector_rpm_source_files]: https://github.com/timberio/vector/tree/master/distribution/rpm

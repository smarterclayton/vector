---
id: "setup/platforms/docker/aws_kinesis_streams"
title: "Send Docker logs to AWS Kinesis Data Streams"
description: "A guide to quickly, and correctly, send Docker logs to AWS Kinesis Data Streams."
platform_name: "docker"
sink_name: "aws_kinesis_streams"
source_name: "docker"
tags: ["category: setup","source: docker","sink: aws_kinesis_streams"]
---

import CodeExplanation from '@site/src/components/CodeExplanation';
import ConfigExample from '@site/src/components/ConfigExample';
import SVG from 'react-inlinesvg';

> "I just wanna, like, send my Docker logs to AWS Kinesis Data Streams -- why is all of this so complicated?"
>
> â€” developers

So you want to send Docker logs to AWS Kinesis Data Streams? Sounds simple! Sadly, it is not.
When you account for x, y, and z, you quickly realize this is no easy endaevor.
Especially for high volume product environments! Fear not! This guide will get
you up and running in minutes.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/guides/setup/platforms/docker/aws_kinesis_streams.md.erb
-->

## What We'll Accomplish In This Guide

<ol className="list--checks list--lg list--semi-bold list--primary list--flush">
  <li>
    Collect Docker container logs.
    <ol>
      <li>Filter which containers you collect them from.</li>
      <li>Automatically merge logs that Docker splits.</li>
      <li>Enrich your logs with useful Docker context.</li>
    </ol>
  </li>
  <li>
    Send logs to AWS Kinesis Data Streams.
    <ol>
      <li>Batch data to maximize throughput.</li>
      <li>Automatically retry failed requests, with backoff.</li>
      <li>Buffer your data in-memory or on-disk for performance and durability.</li>
    </ol>
  </li>
  <li className="list--li--arrow list--li--pink">All in just a few minutes. Let's get started!</li>
</ol>

## How We'll Do It

To send Docker logs to AWS Kinesis Data Streams _properly_, and accomplish all of the items above,
we'll use [Vector][urls.vector_website] and deploy it as a
daemon.

### First, We'll Use Vector

<SVG src="/img/components.svg" width="80%" className="margin-vert--lg" />

Written in [Rust][urls.rust], [Vector][urls.vector_website] is a lightweight
and ultra-fast tool for building observability pipelines. Compared to Logstash
and friends, Vector [improves throughput by ~10X while significanly reducing
CPU and memory usage][urls.vector_performance] and it's the perfect tool for
this task.

### Second, We'll Deploy Vector As A Daemon

<SVG src="/img/deployment-strategies-docker-daemon.svg" />

As shown in the diagram above, the [daemon deployment strategy][docs.strategies.daemon]
is designed for data collection on a single host. In the context of Docker,
Vector is deplyed in it's own container, just like your other services, and it
collects and forwards all data on the host.
[Learn more...][docs.strategies.daemon]

## A Simple Step-By-Step Tutorial

<div className="steps steps--h3">

1. ### Configure Vector

   <ConfigExample
    format="toml"
    path="vector.toml"
    sourceName={"docker"}
    sinkName={"aws_kinesis_streams"} />

2. ### Start the Vector container

   ```bash
   docker run \
     -v $PWD/vector.toml:/etc/vector/vector.toml:ro \
     -v /var/run/docker.sock:/var/run/docker.sock \
     timberio/vector:latest-alpine
   ```

   <CodeExplanation>

   * The `-v $PWD/vector.to...` flag passes your custom configuration to Vector.
   * The `-v /var/run/docke...` flag ensures that Vector has access to the Docker API.
   * The `timberio/vector:latest-alpine` is the default image we've chosen, you are welcome to use [other image variants][docs.platforms.docker#variants].

   </CodeExplanation>

   That's it! Simple and to the point. Hit `ctrl+c` to exit.

</div>


[docs.platforms.docker#variants]: /docs/setup/installation/platforms/docker/#variants
[docs.strategies.daemon]: /docs/setup/deployment/strategies/daemon/
[urls.rust]: https://www.rust-lang.org/
[urls.vector_performance]: https://vector.dev/#performance
[urls.vector_website]: https://vector.dev
